{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpjWIidBK_SP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Definition:\n",
        "K-Nearest Neighbors (KNN) is a non-parametric, instance-based machine learning algorithm that makes predictions based on the similarity (distance) between data points.\n",
        "\n",
        "Working Principle:\n",
        "\n",
        "Given a new input, the algorithm finds the k closest training samples using a distance metric (e.g., Euclidean, Manhattan).\n",
        "\n",
        "The prediction is based on these nearest neighbors.\n",
        "\n",
        "In Classification:\n",
        "\n",
        "Each of the k nearest neighbors votes for a class label.\n",
        "\n",
        "The majority class is assigned to the new input.\n",
        "\n",
        "Example: If k=5 and 3 neighbors belong to class A, 2 to class B → Prediction = Class A.\n",
        "\n",
        "In Regression:\n",
        "\n",
        "The prediction is the average (or weighted average) of the values of the k nearest neighbors.\n",
        "\n",
        "Example: If neighbors have values [10, 12, 14], prediction = 12.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and easy to implement.\n",
        "\n",
        "Works well with smaller datasets.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Computationally expensive for large datasets.\n",
        "\n",
        "Sensitive to irrelevant/noisy features."
      ],
      "metadata": {
        "id": "e4oMg7etLMWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Definition:\n",
        "The Curse of Dimensionality refers to the phenomenon where the performance of algorithms deteriorates as the number of features (dimensions) increases.\n",
        "\n",
        "Why it happens:\n",
        "\n",
        "In high dimensions, data points become sparse.\n",
        "\n",
        "Distance between points becomes less meaningful because all points appear equally far.\n",
        "\n",
        "Effect on KNN:\n",
        "\n",
        "KNN relies heavily on distance measures.\n",
        "\n",
        "In high dimensions, distances become less discriminative, leading to poor classification/regression accuracy.\n",
        "\n",
        "More computational cost for finding neighbors.\n",
        "\n",
        "Solutions:\n",
        "\n",
        "Apply dimensionality reduction (e.g., PCA).\n",
        "\n",
        "Use feature selection to remove irrelevant features.\n",
        "\n",
        "Scale/normalize data."
      ],
      "metadata": {
        "id": "SIRTJ_GULl17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Definition of PCA:\n",
        "\n",
        "PCA is a statistical technique for dimensionality reduction.\n",
        "\n",
        "It transforms original correlated features into a smaller set of uncorrelated principal components.\n",
        "\n",
        "Each component captures maximum variance in the data.\n",
        "\n",
        "Steps of PCA:\n",
        "\n",
        "Standardize data.\n",
        "\n",
        "Compute covariance matrix.\n",
        "\n",
        "Calculate eigenvalues & eigenvectors.\n",
        "\n",
        "Select top components (based on variance).\n",
        "\n",
        "Project data onto new components.\n",
        "\n",
        "PCA vs Feature Selection:\n",
        "\n",
        "Feature Selection: Selects a subset of the original features (no transformation).\n",
        "\n",
        "PCA: Creates new features (linear combinations of originals).\n",
        "\n",
        "Example:\n",
        "\n",
        "Feature selection: Choose “Age” and “Income” from a dataset.\n",
        "\n",
        "PCA: Combine Age + Income into a new principal component."
      ],
      "metadata": {
        "id": "577hsAStLwXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Eigenvalues:\n",
        "\n",
        "Represent the amount of variance captured by each principal component.\n",
        "\n",
        "Larger eigenvalue → More variance explained.\n",
        "\n",
        "Eigenvectors:\n",
        "\n",
        "Define the direction of the new feature axes (principal components).\n",
        "\n",
        "Each eigenvector corresponds to a principal component.\n",
        "\n",
        "Importance in PCA:\n",
        "\n",
        "Eigenvalues: Help decide how many components to retain.\n",
        "\n",
        "Eigenvectors: Provide the new feature space for projecting data.\n",
        "\n",
        "Example:\n",
        "If the first eigenvalue explains 70% variance → keep that component as it carries most information."
      ],
      "metadata": {
        "id": "Mww6rvaiL2o4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Problem:\n",
        "\n",
        "KNN suffers from Curse of Dimensionality in high-dimensional datasets.\n",
        "\n",
        "Solution with PCA:\n",
        "\n",
        "PCA reduces dimensions, removes noise, and retains maximum variance.\n",
        "\n",
        "This makes distance calculations in KNN more meaningful.\n",
        "\n",
        "Pipeline Flow:\n",
        "\n",
        "Step 1: Apply PCA → reduce features.\n",
        "\n",
        "Step 2: Train KNN on reduced dataset.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Faster computation (less features).\n",
        "\n",
        "Better accuracy.\n",
        "\n",
        "Less overfitting.\n",
        "\n",
        "Example:\n",
        "\n",
        "High-dimensional gene dataset → Apply PCA → Use KNN for classification"
      ],
      "metadata": {
        "id": "ZxNI9sHkMODs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "Answer"
      ],
      "metadata": {
        "id": "Af4uUImgN0He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without Scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_without = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn.predict(X_test_scaled)\n",
        "acc_with = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_without)\n",
        "print(\"Accuracy with scaling:\", acc_with)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQRcZY5iN8-Z",
        "outputId": "4431c5cb-7c4e-493f-9231-16584aa607cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "387mDPfAOStM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D39NAE73Oa4h",
        "outputId": "d47a7b65-a4e3-4fd5-89c5-0eb7dbe00907"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "wHKoBJAwOksp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train_pca)\n",
        "y_pred_pca = knn.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy with original scaled data:\", acc_with)\n",
        "print(\"Accuracy with PCA (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEbnlHkpOqdC",
        "outputId": "14c3808d-7fcf-47e9-8b82-cabb3ef40a33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with original scaled data: 0.9629629629629629\n",
            "Accuracy with PCA (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "r2dgao63OwS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test_scaled))\n",
        "\n",
        "# Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test_scaled))\n",
        "\n",
        "print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "print(\"Manhattan Accuracy:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEl2HS-uO0xS",
        "outputId": "737d4b01-4055-4303-9f60-2c80529c5400"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Accuracy: 0.9629629629629629\n",
            "Manhattan Accuracy: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Gene Expression Dataset (High-dimensional data & small samples)\n",
        "\n",
        "Answer:\n",
        "\n",
        "Use PCA to reduce dimensionality:\n",
        "\n",
        "Gene expression data may have thousands of features but only a few samples.\n",
        "\n",
        "PCA reduces features into a smaller set of components capturing max variance.\n",
        "\n",
        "Decide number of components:\n",
        "\n",
        "Plot cumulative explained variance.\n",
        "\n",
        "Select components covering 90–95% variance.\n",
        "\n",
        "Use KNN after PCA:\n",
        "\n",
        "Apply KNN on reduced dataset.\n",
        "\n",
        "Distance metric now works better because data is denser in low dimensions.\n",
        "\n",
        "Evaluate model:\n",
        "\n",
        "Use cross-validation for reliable results.\n",
        "\n",
        "Evaluate with accuracy, precision, recall, F1-score.\n",
        "\n",
        "Justification to stakeholders:\n",
        "\n",
        "Reduces overfitting risk.\n",
        "\n",
        "Handles high-dimensional biomedical data efficiently.\n",
        "\n",
        "Improves interpretability & computational efficiency.\n",
        "\n",
        "A robust real-world pipeline:"
      ],
      "metadata": {
        "id": "dJcI7vGqO6rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Raw Data → PCA → KNN → Evaluation\n"
      ],
      "metadata": {
        "id": "9uLPzdKTPEmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}